<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8"/>
        <title>RACE</title>

        <link href="../race-html.css" rel="stylesheet"/>
    </head>

    <body>
        <div class="navbar">
            <div class=""banner>
                <a href="../index.html" ><img class="logo" src="../images/race128.png"></a>
                <p class="doctitle">RACE Manual</p>
            </div>
            <br/>
            <div class="toc">
                <p class="title">Project Links</p>
                <ul>
                    <li><span class="toc level2"><a href="https://github.com/NASARace/race.git">Repository</a></span></li>
                    <li><span class="toc level2"><a href="http://nasarace.github.io/race">Website</a></span></li>
                </ul>
            </div>
            <div class="toc pagetoc">
                <p class="title">Page</p>
                <ul class="nav-list">
                  <li class="level1 active"><a href="#">Heterogeneous Computing in RACE</a></li>
                  <li class="level2"><a href="#opencl">OpenCL</a></li>
                  <li class="level2"><a href="#cost-benefit-relation">Cost/Benefit Relation</a></li>
                  <li class="level2"><a href="#motivating-example">Motivating Example</a></li>
                  <li class="level2"><a href="#current-state-of-race-cl">Current State of RACE-CL</a></li>
                </ul>
            </div>
            <div class="toc globaltoc">
                <p class="title">Site</p>
                <ul class="nav-list">
                  <li class="level1 nav-header">RACE Manual</li>
                  <li class="level2"><a href="../index.html">About RACE</a></li>
                  <li class="level2 nav-header">Installing RACE</li>
                  <li class="level3"><a href="../installation/prerequisites.html">Prerequisites</a></li>
                  <li class="level3"><a href="../installation/download.html">Downloading RACE</a></li>
                  <li class="level3"><a href="../installation/build.html">Building RACE</a></li>
                  <li class="level3"><a href="../installation/pitfalls.html">Installation Pitfalls</a></li>
                  <li class="level3"><a href="../installation/osx.html">OS X Specifics</a></li>
                  <li class="level2 nav-header">Using RACE</li>
                  <li class="level3"><a href="../usage/running.html">How to Run RACE</a></li>
                  <li class="level3"><a href="../usage/configuration.html">RACE Configuration</a></li>
                  <li class="level3"><a href="../usage/encryption.html">Using Encrypted Configurations</a></li>
                  <li class="level2 nav-header">RACE Design</li>
                  <li class="level3"><a href="overview.html">Design Overview</a></li>
                  <li class="level3"><a href="actors.html">Actors</a></li>
                  <li class="level3"><a href="configuration.html">Runtime Configuration</a></li>
                  <li class="level3"><a href="raceactors.html">RaceActors</a></li>
                  <li class="level3"><a href="channeltopics.html">ChannelTopics</a></li>
                  <li class="level3"><a href="embedded-objects.html">Embedded Objects</a></li>
                  <li class="level3"><a href="connectivity.html">Connecting External Systems</a></li>
                  <li class="level3"><a href="remoting.html">Remote RaceActors and Distributed RACE Applications</a></li>
                  <li class="level3"><a href="remotelauncher.html">RemoteLauncher</a></li>
                  <li class="level3 active"><a href="#">Heterogeneous Computing in RACE</a></li>
                  <li class="level3"><a href="parsing.html">RACE Parsers</a></li>
                  <li class="level3"><a href="archive-replay.html">Archive and Replay</a></li>
                  <li class="level3"><a href="timeseries.html">Time Series Analysis</a></li>
                  <li class="level3"><a href="trajectory.html">Trajectory Infrastructure</a></li>
                  <li class="level3"><a href="rgis.html">RACE GIS Support</a></li>
                  <li class="level3"><a href="worldwind.html">WorldWind Viewer</a></li>
                  <li class="level3"><a href="http-server.html">HttpServer</a></li>
                  <li class="level3"><a href="http-client.html">HttpImportActor</a></li>
                  <li class="level3"><a href="share.html">SHARE - System for Hierarchical Ad hoc Reporting</a></li>
                  <li class="level3"><a href="layers-and-modules.html">Layers and Modules</a></li>
                  <li class="level3"><a href="directory-structure.html">Directory Structure</a></li>
                  <li class="level3"><a href="repository.html">Repository Structure</a></li>
                  <li class="level3"><a href="local-build.html">Local Build Configuration</a></li>
                  <li class="level3"><a href="dds.html">RACE and DDS</a></li>
                  <li class="level3"><a href="extproject.html">Using RACE from External Projects</a></li>
                  <li class="level3"><a href="whyscala.html">Why Scala</a></li>
                  <li class="level2 nav-header">Documentation</li>
                  <li class="level3"><a href="../documentation/overview.html">RACE Documentation Overview</a></li>
                  <li class="level3"><a href="../documentation/presentations.html">Available Slide Presentations</a></li>
                  <li class="level2 nav-header">Annex</li>
                  <li class="level3"><a href="../annex/project-log.html">Project Log</a></li>
                  <li class="level3"><a href="../annex/examples.html">RACE Examples</a></li>
                  <li class="level3"><a href="../annex/known-problems.html">Known Problems</a></li>
                  <li class="level3"><a href="../annex/changes.html">Changes</a></li>
                  <li class="level3"><a href="../annex/todo.html">TODO List</a></li>
                  <li class="level3"><a href="../annex/learning.html">Learning Resources</a></li>
                  <li class="level3"><a href="../annex/glossary.html">Glossary</a></li>
                </ul>
            </div>
        </div>

        <div class="main">
            <h1 id="heterogeneous-computing-in-race" class="title">Heterogeneous Computing in RACE</h1>
            <p>The general theme of RACE is parallel computation. This is most obvious in the choice of using actors as the basic
            programming paradigm, which in a task-parallel execution environment (simultaneous multi-threading or multi-core)
            translates into parallel computation of actors that can work independently.</p>
            <p><a href="remoting.html">Remote Actors</a> or application frameworks such as <a href="share.html">SHARE</a> extend this towards distributed applications executing in
            parallel on multiple network nodes.</p>
            <p>Contemporary computer systems provide more parallel execution capabilities at the hardware level. The primary example
            are modern GPUs that typically feature a number of parallel compute units that is two orders of magnitudes larger than
            even high end CPUs. Even just looking at today&#39;s CPU architectures we see support for data parallel (SIMD) processing
            that is normally under-utilized.</p>
            <p>The main idea behind the <code>race-cl</code> module is to extend the range of support for parallel computation in RACE all the
            way from the macro-cosmos of distributed, networked applications and the cosmos of multi-threading with actors down
            into the micro-cosmos of specialized parallel hardware such as GPUs and SIMD instruction sets.</p>
            <p><img class="center scale50" src="../images/parallel-computing.svg" alt="Parallel Computing Gamut in RACE"></p>
            <p>The challenge is to support such specialized hardware in a programming environment that is as much geared towards cross-
            platform compatibility as a Java Virtual Machine (JVM). Making use of specialized hardware and instruction sets requires
            more control over machine code generation than what such managed-code environments normally provide. In general,
            utilizing specialized hardware is optimization, which should only be performed if required and if an application is
            sufficiently stable since it makes it much harder to maintain and evolve the system.</p>
            
            <h2 id="opencl" class="section"><a class="anchor-link left" href="#opencl"><i class="icofont-laika">&#xef71;</i></a>OpenCL</h2>
            <p>As a middle ground between using hardware agnostic (JVM) code and vendor- and model- specific native code RACE utilizes
            <a href="https://www.khronos.org/opencl/">OpenCL</a> - a standardized, cross-vendor API for heterogeneous computing managed by the <a href="https://www.khronos.org/">Khronos Group</a> (a vendor
            consortium).</p>
            <p>In a nutshell, OpenCL uses <em>kernels</em> - short pieces of mostly linear, C-like code operating on N-dimensional data sets -
            that are runtime compiled and transferred from the host CPU to selected computing devices (such as GPUs).</p>
            <p><img class="center scale50" src="../images/opencl.svg" alt="OpenCL"></p>
            <p>These kernels are then executed in parallel on the selected device, depending on the dimensionality of the data set. In
            addition, kernel code supports built-in vector types that can facilitate fine-grained SIMD processing.</p>
            <p>What makes OpenCL especially attractive is that it abstracts over a wide range of devices and vendors - from discrete
            GPUs to integrated GPUs all the way to even CPUs. Although the latter seems counter-intuitive it can result
            in significant acceleration by means of kernel code making use of specialized instruction sets (such as <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a>) that
            are normally not utilized by cross-platform JVMs.</p>
            <p>However, all this comes at the cost of having to re-factor the data model and having to transfer the data at runtime to
            the device by means of OpenCL <em>buffers</em> in order to make it accessible to the kernel code. Depending on the selected
            device this might involve transferring data to completely different memory systems. Moreover, the higher the potential
            gains from the device are (e.g. discrete GPUs) the more likely it depends on a memory system that is not directly
            accessible by the host/CPU.</p>
            <p>In addition - and unlike accelerated graphics - heterogeneous computation requires round-trips. Once the computation
            on the device has finished the host (CPU) needs to be notified and then needs to transfer the result data back from
            the device memory.</p>
            <p>On the other hand, the asynchronous nature of device/kernel code execution makes the OpenCL programming model especially
            suitable for embedding it into a highly concurrent environment such as RACE - device computations can be encapsulated
            in dedicated actors, not affecting the rest of the system.</p>
            <p>It should also be noted that the use of OpenCL results in a polyglot system (using several implementation languages).
            Due to the simple nature of kernel code (C-99 dialect) this seems acceptable.</p>
            
            <h2 id="cost-benefit-relation" class="section"><a class="anchor-link left" href="#cost-benefit-relation"><i class="icofont-laika">&#xef71;</i></a>Cost/Benefit Relation</h2>
            <p>Based on the OpenCL programming model described above the central question whether a heterogeneous computing
            solution if suitable for a given problem comes down to a simple relation: do the potential gains from parallel (device)
            execution outweigh the additional cost of data transfer to/from the device?</p>
            <p><img class="center scale30" src="../images/hc-hypothesis.svg" alt="cost/benefit of HC"></p>
            <p>Unfortunately, the answer is not that simple. It highly depends on both the problem domain and the targeted device:</p>
            <ol class="arabic">
              <li>how easily can the host data model be translated into a simple N-dimensional device data model?</li>
              <li>what are the actual transfer costs for the chosen memory/buffer types?</li>
            </ol>
            <p>The answer to the second question usually requires experiments that involve micro benchmarks (hard to measure), which
            can yield un-expected results (e.g. an internal GPU being more suitable due to direct memory access).</p>
            <p>The first question can be of even higher relevance since the host data model normally has more complexity (e.g. using
            reference values) than what can/should be mapped to the device. As a result - and to minimize transfer costs - it might
            require to split the data models and to transfer the device data incrementally (i.e. keep it on the device). This
            however will require additional synchronization efforts in order to make sure the overall model stays consistent (e.g.
            to make sure results from the device are consistent with the current host data).</p>
            
            <h2 id="motivating-example" class="section"><a class="anchor-link left" href="#motivating-example"><i class="icofont-laika">&#xef71;</i></a>Motivating Example</h2>
            <p>Despite the cost of hardware optimization there is a strong use case for systems that have to track a large
            number of (mostly) independent objects such as flights, and evaluate properties that involve pairwise computation for
            such objects. This is a quadratic problem that can - depending on computational cost of properties - quickly overwhelm
            scalability.</p>
            <p>A motivating example is realtime detection of loss-of-separation (LOS) for a large number of flights (&gt;100) that are
            each updated with irregular time series (i.e. not synchronously). A typical input source would be local ADS-B receivers
            with update rates ~1Hz.</p>
            <p><img class="center scale60" src="../images/hc-los.svg" alt="loss-of-separation example"></p>
            <p>Each new position update of a flight triggers a chain of computations that can be done in parallel. First, all other
            flight positions have to be extrapolated to the time point of the new update. This operation can be done independently
            for each flight and hence is O(n), but it requires to store and evaluate per-object state (last m positions, intentions).
            Computational cost for this step can vary vastly, based on how the estimation is done and what precision is required.</p>
            <p>Once all flight positions for the update time point have been estimated the system has to do a pair-wise distance
            calculation for theses positions. Again, each of these computations is independent of other pairs, which gives us
            O(n(n-1)/2) complexity. To represent the computational cost of each step, the example diagram uses the
            <a href="https://en.wikipedia.org/wiki/Haversine_formula">haversine formula</a> for distance calculation (which is not warranted in most realistic cases esp. given the uncertainty
            of estimated positions).</p>
            <p>It seems obvious that both position estimation and distance calculation can be performed significantly faster on
            parallel hardware, but the number of parallel operations will easily exceed the level of task parallel execution that
            can be achieved with off-the-shelf CPUs (either through SMT or independent cores).</p>
            
            <h2 id="current-state-of-race-cl" class="section"><a class="anchor-link left" href="#current-state-of-race-cl"><i class="icofont-laika">&#xef71;</i></a>Current State of RACE-CL</h2>
            <p>At this point <code>race-cl</code> is merely an abstraction layer that adds resource management for OpenCL platforms, devices,
            context, programs, command queues, buffers, events and kernels on top of the <a href="https://www.lwjgl.org/">LWJGL</a> library, which in turn is a
            wrapper for the native OpenCL drivers.</p>
            <p>Eventually, <code>CLActor</code> will encapsulate kernel execution and handle the data transfer to/from OpenCL devices.</p>
            <p>Apple deprecated OpenCL support with the release of macOS 10.14 in 2018 in favor of its proprietary <em>Metal</em>
            API. As of 2021 Apple still distributes OpenCL v1.2 with macOS but the tool chain has not seen substantial updates
            and has reliability problems at least on macOS 10.15.7. This puts OpenCLs role as a cross-vendor API for heterogeneous
            computing in question, especially given that there is a overlap with Vulkan compute kernels. RACE might shift to
            Vulkan in the future.</p>
        </div>

    </body>
</html>